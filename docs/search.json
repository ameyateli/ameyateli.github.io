[
  {
    "objectID": "londonmarathon.html",
    "href": "londonmarathon.html",
    "title": "London Marathon",
    "section": "",
    "text": "This data comes from Nicola Rennie’s LondonMarathon R Package containing two data sets scraped from Wikipedia on (1 November 2022) on London Marathon winners, and some general data. Rennie a self proclaimed “enjoyer of endurance races” curated this data set.\nThis data was especially interesting to me as a long distance runner, I hope to one day run a marathon. This data comes from London Marathon data about runners accepted into the race. This is dependent on a few factors: qualifying times (sometimes based on age groups), ballot entry (a lottery entry), running for charity, and many others. Potentially these modes of entry were added as years go by, a reason for the increased accepted runners, or maybe simply marathons have grown in popularity.\nBelow you can see the code and data visualizations of number of accepted runners to the London Marathon from the first year it was established in 1980 to 2020.\n\nlibrary(ggplot2)\nwinners &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-25/winners.csv')\nlondon_marathon &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-25/london_marathon.csv')\n\nggplot(data = london_marathon, aes(x=Year, y=Accepted)) +\n  geom_point() + labs(title =\"Number of Accepted Runners to the London Marathon vs Year\")\n\n\n\n\n\n\n\n\nThe x-axis shows the years of the London Marathon from 1980 to 2020. The y-axis shows the number of accepted runners for that respective year.\nFrom this graph we can see that there is a general positive correlation between the two variables, with the number of runners generally increasing from year to year.\nOne notable thing about this graph is the year 2020, which had 0 accepted runners. Due to COVID-19, the 40th annual London Marathon was cancelled for mass participation. They still, however, had the elite section (a race for professional runners - like Brigid Kosegei who won the women’s elite section) of the race which is not a part of this data for “accepted runners”."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ameya Teli",
    "section": "",
    "text": "Welcome! My name is Ameya Teli and I am a student at Pomona College graduating in 2027. I plan on majoring in Mathematics on the statistic track with a minor in Data Science."
  },
  {
    "objectID": "epl.html",
    "href": "epl.html",
    "title": "Premier League 2021-2022",
    "section": "",
    "text": "Analysis of Goals Scored by Home Teams for each month of the English Premier League in 2021-2022.\nThis data comes from Premier League Match Data 2021-2022 via Evan Gower. Gower collected this data from the official website of the Premier League then was cleaned using google sheets.\nMy mom grew up in Manchester, England and her entire family are avid supporters of one of the two teams based in Manchester: Manchester United. Manchester United and many other professional soccer clubs in the United Kingdom play in a league called the English Premier League (often referred to as just the Premier League or the EPL).\nThe EPL has many devoted fans, an American equivalent could be something like the NFL (in terms of their fan base). Many fans may have superstitions such as what they wear, what they’re doing during the game, and many others. However, a large variable that may affect the way that teams play is whether or not they play their game at home. This could be due to having a so-called “home field advantage”, being on the field that they practice on (sometimes), or maybe because there are more fans there supporting them. This second reason may not be a large factor in their performance because of how many fans each EPL team has, often spread throughout the country. However, it is more likely that the fans who support a team live nearby, as the team can be a sort of emblem of the city.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyverse)\n\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')\n\nhead(soccer)\n\n# A tibble: 6 × 22\n  Date       HomeTeam AwayTeam  FTHG  FTAG FTR    HTHG  HTAG HTR   Referee    HS\n  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 13/08/2021 Brentfo… Arsenal      2     0 H         1     0 H     M Oliv…     8\n2 14/08/2021 Man Uni… Leeds        5     1 H         1     0 H     P Tier…    16\n3 14/08/2021 Burnley  Brighton     1     2 A         1     0 H     D Coote    14\n4 14/08/2021 Chelsea  Crystal…     3     0 H         2     0 H     J Moss     13\n5 14/08/2021 Everton  Southam…     3     1 H         0     1 A     A Madl…    14\n6 14/08/2021 Leicest… Wolves       1     0 H         1     0 H     C Paws…     9\n# ℹ 11 more variables: AS &lt;dbl&gt;, HST &lt;dbl&gt;, AST &lt;dbl&gt;, HF &lt;dbl&gt;, AF &lt;dbl&gt;,\n#   HC &lt;dbl&gt;, AC &lt;dbl&gt;, HY &lt;dbl&gt;, AY &lt;dbl&gt;, HR &lt;dbl&gt;, AR &lt;dbl&gt;\n\n\nBecause the date column in the data set was in day/month/year format and I wanted to group the data in buckets of month in a histogram I created a new data frame called soccer_month which had a new column of month. After converting the date column into a date variable, I extracted the month part of the variable using the function ‘format’.\n\nsoccer_month &lt;- soccer |&gt;\n  mutate(Month = format(as.Date(Date,format=\"%d/%m/%Y\"),\"%m\"))\n\nhead(soccer_month)\n\n# A tibble: 6 × 23\n  Date       HomeTeam AwayTeam  FTHG  FTAG FTR    HTHG  HTAG HTR   Referee    HS\n  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 13/08/2021 Brentfo… Arsenal      2     0 H         1     0 H     M Oliv…     8\n2 14/08/2021 Man Uni… Leeds        5     1 H         1     0 H     P Tier…    16\n3 14/08/2021 Burnley  Brighton     1     2 A         1     0 H     D Coote    14\n4 14/08/2021 Chelsea  Crystal…     3     0 H         2     0 H     J Moss     13\n5 14/08/2021 Everton  Southam…     3     1 H         0     1 A     A Madl…    14\n6 14/08/2021 Leicest… Wolves       1     0 H         1     0 H     C Paws…     9\n# ℹ 12 more variables: AS &lt;dbl&gt;, HST &lt;dbl&gt;, AST &lt;dbl&gt;, HF &lt;dbl&gt;, AF &lt;dbl&gt;,\n#   HC &lt;dbl&gt;, AC &lt;dbl&gt;, HY &lt;dbl&gt;, AY &lt;dbl&gt;, HR &lt;dbl&gt;, AR &lt;dbl&gt;, Month &lt;chr&gt;\n\n\nThis code converts the number for the month into the month name using the function month.name.\n\nmonths_df &lt;- data.frame(\n  month = c(\"August\", \"September\", \"October\", \"November\", \"December\", \"January\", \"February\", \"March\", \"April\", \"May\"), \n  group = c(1:10))\n\n\nsoccer_grouped &lt;- soccer_month |&gt;\n  group_by(Month) |&gt;\n  summarize(Total_Goals = sum(FTHG), num_games = n()) |&gt;\n  mutate(month_num = c(6,7,8,9,10,1,2,3,4,5)) |&gt; \n  mutate(Month_Name = month.name[as.numeric(Month)]) |&gt;\n  mutate(\n    month_faceted = factor(\n      Month_Name, \n      levels = c(\"August\", \"September\", \"October\", \"November\", \"December\", \"January\", \"February\", \"March\", \"April\", \"May\"))) |&gt;\n  arrange(month_faceted)\n\nThe below data visualization shows the number of goals scored by home teams over the course of the season. It is important to note that the EPL season usually runs from August to May. Hence why the factor variable month_faceted was created and is used on the x-axis.\n\nsoccer_grouped |&gt;\n  ggplot(aes(x=month_faceted, y=Total_Goals)) + \n    geom_bar(stat=\"identity\") +\n    labs(\n      title = \"Goals Scored by Home Teams in the EPL\", \n      x= \"Month\", \n      y = \"Goals by Home Team\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\nThis graph shows that for the 2021-2022 season of the EPL the amount of goals scored by home teams fluctuated. It appears to be the highest in December and at lower levels in September, January, and March. Furthermore, this chart shows a count of goals scored, so if there was a month with less games, it would be expected for there to be less goals because there is less opportunities to score. Also, if there was a significance number of goals for a smaller amount of games it wouldn’t show up as significant on this type of graph.\nThis leads me to do a graph that calculates the total goals per game for the home team over the months to compare rather than comparing the total raw sum.\n\nsoccer_grouped &lt;- soccer_grouped |&gt;\n  mutate(goals_per_game = Total_Goals/num_games) \n\n\nsoccer_grouped |&gt;\n  ggplot(\n    aes(\n      x = month_faceted, \n      y = goals_per_game\n      )\n    ) + \n    geom_bar(stat=\"identity\") +\n    labs(\n      title = \"Goals Scored per Game by Home Teams in the EPL\", \n      x= \"Month\", \n      y = \"Goals Scored Per Game\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\nThis chart shows a much more even distribution of goals scored per game in each month unlike the first which had a much greater difference between a few of the months.\nIt would be helpful to add another chart which explores the goals scored by away teams over the course of the season to understand if there is a difference, and if the part of the season impacts the scoring capabilities of each team."
  },
  {
    "objectID": "pj2.html",
    "href": "pj2.html",
    "title": "Project",
    "section": "",
    "text": "Link to Database\n\nlibrary(ggplot2)\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-12')\n\nholiday_movies &lt;- tuesdata$holiday_movies\nholiday_movie_genres &lt;- tuesdata$holiday_movie_genres\n\nholiday_movie_genres\n\n# A tibble: 4,531 × 2\n   tconst    genres   \n   &lt;chr&gt;     &lt;chr&gt;    \n 1 tt0020356 Comedy   \n 2 tt0020823 Drama    \n 3 tt0020823 Romance  \n 4 tt0020985 Comedy   \n 5 tt0020985 Drama    \n 6 tt0021268 Comedy   \n 7 tt0021377 Comedy   \n 8 tt0021377 Romance  \n 9 tt0021381 Adventure\n10 tt0021381 Crime    \n# ℹ 4,521 more rows\n\nholiday_movies\n\n# A tibble: 2,265 × 14\n   tconst   title_type primary_title original_title  year runtime_minutes genres\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt; \n 1 tt00203… movie      Sailor's Hol… Sailor's Holi…  1929              58 Comedy\n 2 tt00208… movie      The Devil's … The Devil's H…  1930              80 Drama…\n 3 tt00209… movie      Holiday       Holiday         1930              91 Comed…\n 4 tt00212… movie      Holiday of S… Prazdnik svya…  1930              83 Comedy\n 5 tt00213… movie      Sin Takes a … Sin Takes a H…  1930              81 Comed…\n 6 tt00213… movie      Sinners' Hol… Sinners' Holi…  1930              60 Adven…\n 7 tt00230… movie      Husband's Ho… Husband's Hol…  1931              70 Drama \n 8 tt00248… movie      Beggar's Hol… Beggar's Holi…  1934              60 Crime…\n 9 tt00250… movie      Cowboy Holid… Cowboy Holiday  1934              56 Weste…\n10 tt00250… movie      Death Takes … Death Takes a…  1934              79 Drama…\n# ℹ 2,255 more rows\n# ℹ 7 more variables: simple_title &lt;chr&gt;, average_rating &lt;dbl&gt;,\n#   num_votes &lt;dbl&gt;, christmas &lt;lgl&gt;, hanukkah &lt;lgl&gt;, kwanzaa &lt;lgl&gt;,\n#   holiday &lt;lgl&gt;\n\n\nFirst Plot - What genre do movies with love in the title fall under?\n\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\ncolnames(holiday_movies)\n\n [1] \"tconst\"          \"title_type\"      \"primary_title\"   \"original_title\" \n [5] \"year\"            \"runtime_minutes\" \"genres\"          \"simple_title\"   \n [9] \"average_rating\"  \"num_votes\"       \"christmas\"       \"hanukkah\"       \n[13] \"kwanzaa\"         \"holiday\"        \n\n#holiday_movies$simple_title\n\nlove &lt;- holiday_movies|&gt;\n  filter(str_detect(simple_title, \"love\"))\nlove\n\n# A tibble: 32 × 14\n   tconst   title_type primary_title original_title  year runtime_minutes genres\n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt; \n 1 tt00528… movie      Holiday for … Holiday for L…  1959             103 Comedy\n 2 tt00855… tvMovie    The Gift of … The Gift of L…  1983              96 Drama \n 3 tt00999… tvMovie    The Kid Who … The Kid Who L…  1990             118 Drama \n 4 tt01370… tvMovie    A Holiday fo… Christmas in …  1996             105 Drama…\n 5 tt04142… tvMovie    Love Hina Ch… Rabu Hina kur…  2000              46 Anima…\n 6 tt04185… movie      Christmas in… Christmas in …  2004             118 Comed…\n 7 tt10623… tvMovie    Christmas Lo… Christmas Lov…  2019              86 Comed…\n 8 tt11171… tvMovie    Our Christma… Our Christmas…  2019              82 Drama…\n 9 tt11666… tvMovie    Inn Love by … Inn for Chris…  2020              88 Comed…\n10 tt12418… movie      A Christmas … A Christmas T…  2020              70 Roman…\n# ℹ 22 more rows\n# ℹ 7 more variables: simple_title &lt;chr&gt;, average_rating &lt;dbl&gt;,\n#   num_votes &lt;dbl&gt;, christmas &lt;lgl&gt;, hanukkah &lt;lgl&gt;, kwanzaa &lt;lgl&gt;,\n#   holiday &lt;lgl&gt;\n\nlove_genre &lt;- love|&gt;\n  mutate(genre_list = genres)|&gt;\n  separate(genres, c('first','second', 'third'))\nlove_genre\n\n# A tibble: 32 × 17\n   tconst    title_type primary_title original_title  year runtime_minutes first\n   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;\n 1 tt0052897 movie      Holiday for … Holiday for L…  1959             103 Come…\n 2 tt0085593 tvMovie    The Gift of … The Gift of L…  1983              96 Drama\n 3 tt0099930 tvMovie    The Kid Who … The Kid Who L…  1990             118 Drama\n 4 tt0137000 tvMovie    A Holiday fo… Christmas in …  1996             105 Drama\n 5 tt0414243 tvMovie    Love Hina Ch… Rabu Hina kur…  2000              46 Anim…\n 6 tt0418599 movie      Christmas in… Christmas in …  2004             118 Come…\n 7 tt106234… tvMovie    Christmas Lo… Christmas Lov…  2019              86 Come…\n 8 tt111713… tvMovie    Our Christma… Our Christmas…  2019              82 Drama\n 9 tt116669… tvMovie    Inn Love by … Inn for Chris…  2020              88 Come…\n10 tt124182… movie      A Christmas … A Christmas T…  2020              70 Roma…\n# ℹ 22 more rows\n# ℹ 10 more variables: second &lt;chr&gt;, third &lt;chr&gt;, simple_title &lt;chr&gt;,\n#   average_rating &lt;dbl&gt;, num_votes &lt;dbl&gt;, christmas &lt;lgl&gt;, hanukkah &lt;lgl&gt;,\n#   kwanzaa &lt;lgl&gt;, holiday &lt;lgl&gt;, genre_list &lt;chr&gt;\n\nlove_movies &lt;- inner_join(love_genre, holiday_movie_genres, by = \"tconst\")\nlove_movies\n\n# A tibble: 66 × 18\n   tconst    title_type primary_title original_title  year runtime_minutes first\n   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;\n 1 tt0052897 movie      Holiday for … Holiday for L…  1959             103 Come…\n 2 tt0085593 tvMovie    The Gift of … The Gift of L…  1983              96 Drama\n 3 tt0099930 tvMovie    The Kid Who … The Kid Who L…  1990             118 Drama\n 4 tt0137000 tvMovie    A Holiday fo… Christmas in …  1996             105 Drama\n 5 tt0137000 tvMovie    A Holiday fo… Christmas in …  1996             105 Drama\n 6 tt0137000 tvMovie    A Holiday fo… Christmas in …  1996             105 Drama\n 7 tt0414243 tvMovie    Love Hina Ch… Rabu Hina kur…  2000              46 Anim…\n 8 tt0414243 tvMovie    Love Hina Ch… Rabu Hina kur…  2000              46 Anim…\n 9 tt0414243 tvMovie    Love Hina Ch… Rabu Hina kur…  2000              46 Anim…\n10 tt0418599 movie      Christmas in… Christmas in …  2004             118 Come…\n# ℹ 56 more rows\n# ℹ 11 more variables: second &lt;chr&gt;, third &lt;chr&gt;, simple_title &lt;chr&gt;,\n#   average_rating &lt;dbl&gt;, num_votes &lt;dbl&gt;, christmas &lt;lgl&gt;, hanukkah &lt;lgl&gt;,\n#   kwanzaa &lt;lgl&gt;, holiday &lt;lgl&gt;, genre_list &lt;chr&gt;, genres &lt;chr&gt;\n\nggplot(love_movies, aes(x = genres))+ geom_bar(stat = \"count\")+labs(title = \"Genres of Holiday Movies with Love in the Title\", x = \"Genre\")\n\n\n\n\n\n\n\n\nThis plot shows that the most prevalent genre’s for holiday movies about love are Romance, Family, Drama, and Comedy.\n2nd Plot - Genres of Holiday Movies\n\nggplot(holiday_movie_genres, aes(x = genres)) + \n  geom_bar(stat = \"count\") + \n  labs(title = \"Genres of Holiday Movies\", x = \"Genre\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n3rd Plot -"
  },
  {
    "objectID": "shake.html",
    "href": "shake.html",
    "title": "Shakespeare Textual Analysis",
    "section": "",
    "text": "This my textual analysis of Shakespeare dialogue in his tragedy “Romeo and Juliet”.\nThis data was curated by Nicola Rennie from The Complete Works of William Shakespeare the Web’s first edition of the Complete Works of William Shakespeare.\n\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/romeo_juliet.csv')\n\nHow much is death mentioned in Romeo and Juliet?\n\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndeathrj &lt;- romeo_juliet |&gt;\n filter(str_detect(dialogue, \"death\")) \n  \n\nggplot(deathrj, aes(x = character)) +\n  geom_bar(stat = \"count\") + \n  labs(title = \"Which character mentions death the most?\", x = \"Character\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\nlibrary(stringr)\n\nact_num &lt;- deathrj |&gt;\n  mutate(act = str_extract(act, \"(?&lt;=\\\\s)\\\\S+\"))  \n  \nggplot(act_num, aes(x = act)) + \n  geom_bar(stat = \"count\") + \n  labs(title = \"Which act has most mentions of death?\", x = \"Act\") \n\n\n\n\n\n\n\n\nThis first graph shows that Romeo has the most instances of the word “death” in his dialogue and Friar Laurence had the second most in his dialogue. This is interesting because it makes sense that Romeo says death the most as he eventually is one of those who die in this tragedy and Friar Laurence is the one to encourage this from selling a sleeping potion. As Romeo has more dialogue presumably as one of the more main characters it makes sense that he has more mentions by count versus the Apothecary who sells Romeo poison that will kill him.\nRomeo and Juliet die in Act 5 Scene 3, so having the most mentions of death in Act 3 is interesting. Maybe there is some foreshadowing happening here or something else significant which we can look through in the next plot.\nThis plot will analyze specifically dialogue in Act 3 looking for exclamations and question marks which indicate a strong emotion in the dialogue.\n\nfiltered_act_num &lt;- act_num |&gt;\n  filter(act == \"III\")|&gt;\n  mutate(strong_punctuation = str_detect(dialogue, \"!+|\\\\?+\")) |&gt;\n  mutate(scene = str_extract(scene, \"(?&lt;=\\\\s)\\\\S+\"))  \n  \n\nggplot(filtered_act_num, aes(x = scene, fill = strong_punctuation))+\n  geom_bar(position = \"fill\") + \n  labs(title = \"Proportion of strong punctuation marks across scenes\", x = \"Scene\", y = \"Proportion\")\n\n\n\n\n\n\n\n\nThis shows that there is a greater proportion of strong punctuation in Scene 5 than the rest. So potentially the strongest emotions in relation to death occur here? We will also check which scene had the most mentions of death below.\n\nggplot(filtered_act_num, aes(x = scene)) +\n  geom_bar(stat = \"count\") + \n  labs(title = \"Mentions of death across scenes in Act III\", x = \"Scene\", y = \"Count\")\n\n\n\n\n\n\n\n\nMaybe this indicates that the mentions of death in the other scenes are less emotional and thus use less strong punctuation. We will look for the following word, symbol, or space after death, to see how they compare.\n\nfiltered_act_num &lt;- act_num |&gt;\n  filter(act == \"III\")|&gt;\n  mutate(word_after_death = str_extract(dialogue, \"death\\\\s*\\\\S+\"))\n\nggplot(filtered_act_num, aes(x = word_after_death))+\n  geom_bar(stat = \"count\")+\n  labs(title = \"Death and the Word/Character after in Act III\", \n       x = \"Death + Word\", \n       y = \"Count\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThe most common thing following death is a comma, indicating that death ends a thought of most of the dialogue in Act III. This is interesting if we wanted to analyze the style that Shakespeare uses to foreshadow the impending doom of Romeo and Juliet and those others who face tragic endings.\n\nfiltered_act_num\n\n# A tibble: 25 × 6\n   act   scene     character      dialogue          line_number word_after_death\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;           \n 1 III   Scene I   Mercutio       cat, to scratch …        1525 death!          \n 2 III   Scene I   Benvolio       Stand not amazed…        1560 death,          \n 3 III   Scene I   Benvolio       Cold death aside…        1590 death aside,    \n 4 III   Scene II  Juliet         Than the death-d…        1676 death-darting   \n 5 III   Scene II  Juliet         Some word there …        1739 death,          \n 6 III   Scene II  Juliet         Hath slain ten t…        1745 &lt;NA&gt;            \n 7 III   Scene II  Juliet         But with a rear-…        1752 death,          \n 8 III   Scene II  Juliet         In that word's d…        1757 death;          \n 9 III   Scene II  Juliet         And death, not R…        1768 death,          \n10 III   Scene III Friar Laurence Not body's death…        1786 death,          \n# ℹ 15 more rows\n\n\nUpon viewing the first few lines of this data frame we can see that “Tybalt’s death” is mentioned a great amount, so perhaps the reason death is mentioned so much in this Act is because this character actually did die.\n\n\nFuture analysis on how family names associate with mentions of death:\nThese next two plots analyze which characters mention the Montague’s and the Capulet’s the most - the two rivaling families within Romeo and Juliet.\n\nromeo_juliet |&gt;\n  mutate(dialogue_lower = str_to_lower(dialogue))|&gt;\n  filter(str_detect(dialogue_lower, \"montague\")) |&gt;\n  filter(character != \"[stage direction]\") |&gt;\n  ggplot(aes(x = character)) +\n  geom_bar(stat = \"count\") + \n  labs(title = \"Which character mentions the Montague family the most?\", x = \"Character\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\nromeo_juliet |&gt;\n  mutate(dialogue_lower = str_to_lower(dialogue))|&gt;\n  filter(str_detect(dialogue_lower, \"capulet\")) |&gt;\n  filter(character != \"[stage direction]\") |&gt;\n  ggplot(aes(x = character)) +\n  geom_bar(stat = \"count\") + \n  labs(title = \"Which character mentions the Capulet family the most?\", x = \"Character\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\n\nThese two plots show that Juliet mentions the Montague family the most and Benvolio, Prince, and Romeo mention the Capulet family the most. This is interesting because Romeo and Juliet have the most dialogue presumably as they are main characters so they mention the family names the most."
  },
  {
    "objectID": "simstudy.html",
    "href": "simstudy.html",
    "title": "Permutation Test",
    "section": "",
    "text": "Source: openintro package R/data-sex_discrimination.R\nObjective: See if there is a correlation between gender and being promoted from a study in the 1970s about bank manager recommendations based on sex. We want to show that the proportion of women who are promoted is less than men and that this is significant.\nOur null hypothesis is that gender does not affect the likelihood that men or women are promoted.\n\nlibrary(openintro)\n\n\nsex_discrimination\n\n# A tibble: 48 × 2\n   sex   decision\n   &lt;fct&gt; &lt;fct&gt;   \n 1 male  promoted\n 2 male  promoted\n 3 male  promoted\n 4 male  promoted\n 5 male  promoted\n 6 male  promoted\n 7 male  promoted\n 8 male  promoted\n 9 male  promoted\n10 male  promoted\n# ℹ 38 more rows\n\n\nThe following code is to calculate the summary statistic for our data.\n\nlibrary(dplyr)\n#the number of males and females in the study\nnmale = sum(sex_discrimination$sex == \"male\")\nnfemale = sum(sex_discrimination$sex ==\"female\")\n\n#the number of males and females that were promoted\n\nmales &lt;- filter(sex_discrimination, sex == \"male\")\n\nmale_promoted = sum(males$decision == \"promoted\")\n\nfemales &lt;- filter(sex_discrimination, sex == \"female\")\n\nfemale_promoted = sum(females$decision == \"promoted\")\n\n#calculating the proportion of promotion with regard to gender \n\nprop_male &lt;- male_promoted/nmale\n\nprop_female &lt;- female_promoted/nfemale\n\n#create a dataframe with these summary statistics\n\ngender_data &lt;- data_frame(\n  Gender = c(\"male\", \"female\"), \n  Count = c(nmale, nfemale),\n  Promoted = c(male_promoted, female_promoted), \n  Proportion = c(prop_male, prop_female)\n)\n\ngender_data\n\n# A tibble: 2 × 4\n  Gender Count Promoted Proportion\n  &lt;chr&gt;  &lt;int&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 male      24       21      0.875\n2 female    24       14      0.583\n\n#Calculating the different between the proportion of males that were promoted and females that were promoted\nobs_diff &lt;- prop_male - prop_female\nobs_diff\n\n[1] 0.2916667\n\n\nNow we generate a null sampling distribution\n\nlibrary(dplyr)\nlibrary(purrr)\n\n#function to find the proportion of a specific gender which are given a promotion \n\nfind_prop &lt;- function(gender, data, promote_col){\n  gender_specific &lt;- filter(data, sex == gender)\n  n_gender &lt;- sum(data$sex == gender)\n  gender_promoted &lt;- sum(gender_specific[[promote_col]] == \"promoted\" )\n  return(gender_promoted/n_gender)\n}\n\n#test that find_prop works \nfind_prop(\"male\", sex_discrimination, \"decision\")\n\n[1] 0.875\n\n#we care about whether or not females are promoted the same as or less than the males so we care about the sign of the difference (hence no absolute value)\nfind_prop_diff &lt;- function(data, promote_col){\n  find_prop(\"male\", data, promote_col) - find_prop(\"female\", data, promote_col)\n}\n\nfind_prop_diff(sex_discrimination, \"decision\")\n\n[1] 0.2916667\n\n#function to permute the data & calculate the the proportion difference \nperm_data &lt;- function(data){\n  data &lt;- data |&gt;\n    mutate(promote_perm = sample(decision, replace = FALSE)) \n  summarize(data, obs_prop_diff = find_prop_diff(data, \"promote_perm\"))\n}\n\n\nlibrary(ggplot2)\n\n#mapping the permutation \nreplication &lt;- function(rep, data){\n  map_dfr(1:rep, ~perm_data(data))\n}\n\nset.seed(4747)\nperm_stats &lt;- replication(100, sex_discrimination)\n\nggplot(perm_stats, aes(x = obs_prop_diff)) + \n  geom_histogram(binwidth = .05) + \n  geom_vline(aes(xintercept = obs_diff), color = \"red\") +\n  labs(title = \"Null Distribution of Promotion Differences by Gender\", \n       x = \"Proportion Difference (Permuted Data)\",\n       y = \"Frequency\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is a graph of the null sampling distribution, that is we permuted the data many times switching the labels for men and women and promoted vs. not promoted and plotted this on the above histogram. The red vertical line shows our observed difference from the initial the sex_discrimination data from the study.\n\nsum(perm_stats&gt;=obs_diff)\n\n[1] 2\n\n\nThere are only two entries from our null sampling distribution that are greater than or equal to our observed difference from the sampling distribution. This gives us a p value of 0.02 which is very small. Small enough for us to reject our null hypothesis. Thus the gender of the person does make a difference on whether or not they were promoted. More specifically to the point of this simulation study, the proportion of females promoted was significantly fewer than the proportion of men who were promoted. Thus gender, specifically being a female plays a significant role."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "ameyateli.github.io",
    "section": "",
    "text": "library(dplyr)\nlibrary(dbplyr)\n\n\nlibrary(RMariaDB)\nlibrary(DBI)\ncon_wai &lt;- dbConnect(\n  MariaDB(), \n  host = \"scidb.smith.edu\",\n  user = \"waiuser\", \n  password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931"
  },
  {
    "objectID": "sqlproject.html",
    "href": "sqlproject.html",
    "title": "SQL Project",
    "section": "",
    "text": "This data comes from the WAI-Database. (https://www.science.smith.edu/wai-database/home/about/)\nThe goal of this project is to: (1) Recreate Figure 1 from Voss (2020) (2) Create my own graph that groups by a demographic variable in a particular study rather than all absorbances by study (for this project I will use sex)\n\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\nlibrary(RMariaDB)\nlibrary(DBI)\ncon_wai &lt;- dbConnect(\n  MariaDB(), \n  host = \"scidb.smith.edu\",\n  user = \"waiuser\", \n  password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n\n# collect(Measurements)\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nThis data shows that each study recorded absorbance of each subject many many times.\nTo recreate Figure 1 from Voss (2020) we must find the mean absorbance for each ear in each study, then plot it against the frequency. So I have to group the data by the study and frequency.\n\nSELECT \n  Identifier, \n  Frequency, \n  AVG(Absorbance) AS Mean_Absorbance\nFROM Measurements \nWHERE Identifier IN (\n\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\", \"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\", \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\"\n) \nAND Absorbance &gt;= 0\nAND Frequency BETWEEN 200 AND 8000\nGROUP BY Identifier, Frequency\n\nThe table that has the information to add to the legend such as the author list and year is PI_Into.\nThis code creates a data frame with the information we need for the legend, including the Authors, the year, and the instrument used in the study from PI_Info. Then it is joined with Measurements using LEFT JOIN because we want all the ear entries to have an associated legend entry.\n\nSELECT \nAuthorsShortList, \nYear, \nMeasurements.Instrument, \nCOUNT(DISTINCT SubjectNumber, Ear) AS Unique_Ears, \nPI_Info.Identifier\nFROM PI_Info\nLEFT JOIN Measurements ON PI_Info.Identifier = Measurements.Identifier\nWHERE PI_Info.Identifier IN (\n\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\", \"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\", \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\"\n) \nGROUP BY PI_Info.Identifier, Instrument\n\nHere I create a new data frame from joining table_one and legend to add the data from legend that we was not present in our initial filtering of table_one. Furthermore, the graph by Voss, instead of having entries with “other” for the Instrument it has “not commercial system”, so I replaced those. The last mutate is to create the legend entries like that of the original graph.\n\ngraph_data &lt;- table_one |&gt;\n  left_join(legend, by = c(\"Identifier\" = \"Identifier\")) |&gt;\n    mutate(Instrument = ifelse(Instrument == \"Other\", \"not commercial system\", Instrument)) |&gt;\n  mutate(name = paste0(AuthorsShortList, \"(\", Year, \") \",\"N=\", Unique_Ears, \"; \", Instrument)) \n\nHere I create the graph using ggplot using color = name to make each name have a different associated color rather than study allowing it to be in the legend.\n\ngraph_data |&gt;\n  ggplot(aes(\n      x = Frequency, \n      y = Mean_Absorbance, \n      color = name)) + \n  geom_line(size = 0.5) +\n  theme_minimal() + \n  theme(\n    legend.position = \"right\",legend.key.size = unit(0.1, 'cm') \n  ) + \n  labs(\n    x = \"Frequency (Hz)\", \n    y = \"Mean Absorbance\", \n    title = \"Mean absorbance from each publication in WAI database\") + \n  ylim(0,1) + \n  scale_x_log10()\n\n\n\n\n\n\n\n\nThis graph is meant to be a recreation of the one by Voss. It shows that as frequency increases and is around 3000 Hz, the mean absorbence of most participants was at its peak. So from the participants in these studies, the best absorption their ears were able to do was at this frequency. Also very quicly after this there is a steep drop in the graph, which may indicate that after this ideal frequency of 3000 Hz it is harder for ears to absorb.\nPart 2 of this project explores specifically the study done by Lewis and Neely in 2015. In their study they track sex as a variable. The goal of this section is to create a graph like the one above, but instead of having a line for each study, have a line for each gender in this specific study.\n\nSELECT * \nFROM Subjects\nWHERE Identifier = \"Lewis_2015\"\nLIMIT 0, 10\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nLewis_2015\n1\n2\n25\nAdult\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):2.8989e-05 m^2, shallow placement(session2):4.1712e-05 m^2; Left ear data NA\n\n\nLewis_2015\n2\n2\n22\nAdult\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):3.1677e-05 m^2, shallow placement(session2):4.1669e-05 m^2; Left ear data NA\n\n\nLewis_2015\n3\n2\n37\nAdult\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):2.2295e-05 m^2, shallow placement(session2):2.9556e-05 m^2; Left ear data NA\n\n\nLewis_2015\n4\n2\n21\nAdult\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):2.6597e-05 m^2, shallow placement(session2):3.3578e-05 m^2; Left ear data NA\n\n\nLewis_2015\n5\n2\n25\nAdult\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):3.6921e-05 m^2, shallow placement(session2):4.7702e-05 m^2; Left ear data NA\n\n\nLewis_2015\n6\n2\n45\nAdult\nMale\nBlack\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):3.1565e-05 m^2, shallow placement(session2):3.2389e-05 m^2; Left ear data NA;Subject identified as black or african american\n\n\nLewis_2015\n7\n2\n33\nAdult\nMale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):2.2472e-05 m^2, shallow placement(session2):2.8156e-05 m^2; Left ear data NA\n\n\nLewis_2015\n8\n2\n26\nAdult\nFemale\nAsian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):3.3991e-05 m^2, shallow placement(session2):3.7724e-05 m^2; Left ear data NA;Subject did not differentiate between Chinese and Asian\n\n\nLewis_2015\n9\n2\n25\nAdult\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):3.2203e-05 m^2, shallow placement(session2):3.9391e-05 m^2; Left ear data NA\n\n\nLewis_2015\n10\n2\n27\nAdult\nFemale\nCaucasian\nNonHispanic\nNormal\nNormal\nRight ear: deep probe placement(session1):1.8787e-05 m^2, shallow placement(session2):3.0184e-05 m^2; Left ear data NA\n\n\n\n\n\nFor this next visualizations I will focus on specifically Lewis_2015, which has data on the gender of it’s participants.\nTo do this I first create a new data frame table_two which is all the frequencies and mean absorbances for the Lewis_2015 study joined with Measurements to obtain the Mean_Absorbance for each frequency as observed in part 1, however, this section adds the additional group of “Sex”.\n\nSELECT \n  Measurements.Identifier, \n  Frequency, \n  AVG(Absorbance) AS Mean_Absorbance, \n  Sex, \n  AuthorsShortList\nFROM Measurements \nJOIN PI_Info ON Measurements.Identifier = PI_Info.Identifier\nJOIN Subjects ON Subjects.SubjectNumber = Measurements.SubjectNumber \nWHERE Measurements.Identifier = \"Lewis_2015\" \nAND Absorbance &gt;= 0\nAND Frequency BETWEEN 200 AND 8000\nGROUP BY Sex, Identifier, Frequency\n\nUsing this data frame I can use ggplot to plot the data.\n\ntable_two |&gt;\nggplot(\n  aes(\n    x = Frequency, \n    y = Mean_Absorbance, \n    color = Sex)) + \n  geom_line() + \n  labs(title = \"Mean Absorbance vs. Frequency based on gender in Lewis and Neely's 2015 study\", y = \"Mean Absorbance\", x = \"Frequency (Hz)\")\n\n\n\n\n\n\n\n\nThis graph has three different colored lines. One for female, male and unknown (possibly all the participants whose genders were not recorded). This graph shows that all three of the sexes have similar Mean Absorbances at frequencies. However, it appears that the unknown gender has much more fluctuations than the other two. This could be due to the mixture of genders that are in this group, or that it may be larger than the others. Zooming into the female and male lines, we can see that the male line is generally above the female line from 1000 Hz to 6000 Hz. However, at most other points the lines are almost identical."
  }
]